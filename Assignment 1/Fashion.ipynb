{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sn\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating train and test data\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "X_train_new = X_train.reshape(\n",
    "    (X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "X_test_new = X_test.reshape(\n",
    "    (X_test.shape[0], X_test.shape[1] * X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging sample images\n",
    "\n",
    "wandb.init(project='cs6910-a1')\n",
    "done = set()\n",
    "for x, y in zip(X_train, Y_train):\n",
    "    if y not in done:\n",
    "        done.add(y)\n",
    "        wandb.log({'img': wandb.Image(x, caption='Set ' + str(y))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that generates a Feedforward Neural Network with given hyperparameters\n",
    "\n",
    "class FNNClassifier:\n",
    "    def __init__(self,\n",
    "                 layer_size,\n",
    "                 num_layers,\n",
    "                 activation = 'ReLU',\n",
    "                 optimizer = 'adam',\n",
    "                 weight_decay = 0.0001,\n",
    "                 batch_size = 200,\n",
    "                 learning_rate = 0.001,\n",
    "                 num_epochs = 200,\n",
    "                 weight_init = 'Xavier',\n",
    "                 loss = 'cross_entropy'):\n",
    "        self.activation = activation  # 'sigmoid', 'tanh', 'ReLU'\n",
    "        self.optimizer = optimizer  # 'normal', 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'\n",
    "        self.weight_decay = weight_decay  # L2 regularization hyperparameter\n",
    "        self.batch_size = batch_size  # Batch size\n",
    "        self.learning_rate = learning_rate  # Learning Rate\n",
    "        self.num_epochs = num_epochs  # Number of epochs\n",
    "        self.weight_init = weight_init  # 'random', 'Xavier'\n",
    "        self.loss = loss  # 'cross_entropy', 'square'\n",
    "        self.n = 100\n",
    "        self.K = 10\n",
    "        self.L = num_layers\n",
    "        self.N = layer_size\n",
    "        self.layer_sizes = np.zeros((num_layers + 2))\n",
    "        self.layer_sizes[1:num_layers + 1] = layer_size\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.a = []\n",
    "        self.h = []\n",
    "\n",
    "    def act(self, z):\n",
    "        if (self.activation == 'sigmoid'):\n",
    "            return np.where(z > 0, 1 / (1 + np.exp(-z)),\n",
    "                            np.exp(z) / (1 + np.exp(z)))\n",
    "        elif (self.activation == 'tanh'):\n",
    "            return np.where(z > 0, (1 - np.exp(-2 * z)) / (1 + np.exp(-2 * z)),\n",
    "                            (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1))\n",
    "        elif (self.activation == 'ReLU'):\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "    def deriv_act(self, z):\n",
    "        if (self.activation == 'sigmoid'):\n",
    "            return self.act(z) * (1 - self.act(z))\n",
    "        elif (self.activation == 'tanh'):\n",
    "            return 1 - self.act(z)**2\n",
    "        elif (self.activation == 'ReLU'):\n",
    "            return np.maximum(np.sign(z), np.zeros(z.shape))\n",
    "\n",
    "    def oact(self, z):\n",
    "        temp = np.exp(z) / np.exp(z).sum(axis=0)\n",
    "        return np.where(np.isnan(temp).sum(axis=0) == 0, temp, z == z.max(axis=0))\n",
    "\n",
    "    def loss_calc(self, Y, Y_pred):\n",
    "        if (self.loss == 'cross_entropy'):\n",
    "            return -np.sum(np.log2(Y_pred[np.arange(Y.shape[0]), Y]))\n",
    "        elif (self.loss == 'square'):\n",
    "            return (np.sum(Y_pred**2) + Y.shape[0] -\n",
    "                    2 * np.sum(Y_pred[np.arange(Y.shape[0]), Y])) / 2\n",
    "\n",
    "    def accuracy(self, Y, Y_pred):\n",
    "        return np.sum(Y_pred == Y) / Y.shape[0]\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        self.a = [np.zeros((1, 1))]\n",
    "        self.h = [X]\n",
    "\n",
    "        for i in range(1, self.L + 1):\n",
    "            self.a.append((self.b[i].T + (self.W[i].T @ self.h[i - 1]).T).T)\n",
    "            self.h.append(self.act(self.a[i]))\n",
    "        self.h[-1] = self.oact(self.a[-1])\n",
    "        return self.h[-1]\n",
    "\n",
    "    def back_prop(self, Y_pred, ey):\n",
    "        self.gradW, self.gradB, self.grada = [], [], []\n",
    "        self.gradh = [np.zeros((1, 1))]\n",
    "        if (self.loss == 'cross_entropy'):\n",
    "            self.grada.append(-(ey - Y_pred))\n",
    "        elif (self.loss == 'square'):\n",
    "            self.grada.append((ey - Y_pred) * Y_pred -\n",
    "                              (Y_pred @ (ey - Y_pred).T) @ Y_pred)\n",
    "        for i in range(self.L, 0, -1):\n",
    "            self.gradW.append((self.grada[self.L - i] @ self.h[i - 1].T).T)\n",
    "            self.gradB.append(self.grada[self.L - i].sum(axis=1))\n",
    "            self.gradh.append(self.W[i] @ self.grada[self.L - i])\n",
    "            self.grada.append(self.gradh[self.L - i + 1] *\n",
    "                              self.deriv_act(self.a[i - 1]))\n",
    "\n",
    "        self.gradW.append(np.zeros((self.N, self.N)))\n",
    "        self.gradB.append(np.zeros(self.N))\n",
    "\n",
    "        return\n",
    "\n",
    "    def wb_init(self, num):\n",
    "        if (self.weight_init == 'random'):\n",
    "            for i in range(self.L):\n",
    "                self.W.append(np.random.randn(self.N, self.N))\n",
    "                self.b.append(np.random.randn(self.N))\n",
    "            self.W.append(np.random.randn(self.N, self.K))\n",
    "            self.W[1] = np.random.randn(num, self.W[1].shape[1])\n",
    "            self.b.append(np.random.randn(self.K))\n",
    "\n",
    "        elif (self.weight_init == 'Xavier'):\n",
    "            for i in range(self.L):\n",
    "                self.W.append(\n",
    "                    np.random.normal(0, np.sqrt(1 / self.N), (self.N, self.N)))\n",
    "                self.b.append(np.random.normal(0, np.sqrt(1 / self.N), self.N))\n",
    "            self.W.append(\n",
    "                np.random.normal(0, np.sqrt(2 / (self.N + self.K)),\n",
    "                                 (self.N, self.K)))\n",
    "            self.W[1] = np.random.normal(\n",
    "                0, np.sqrt(2 / (num + self.W[1].shape[1])),\n",
    "                (num, self.W[1].shape[1]))\n",
    "            self.b.append(np.random.normal(0, np.sqrt(1 / self.K), self.K))\n",
    "\n",
    "    def grad_desc(self, X, Y, X_val = 0, Y_val = 0, validate = False):\n",
    "\n",
    "        self.wb_init(X.shape[0])\n",
    "\n",
    "        W_0, b_0 = [], []\n",
    "        for i in range(self.L):\n",
    "            W_0.append(np.zeros((self.N, self.N)))\n",
    "            b_0.append(np.zeros(self.N))\n",
    "        W_0.append(np.zeros((self.N, self.K)))\n",
    "        W_0[1] = np.zeros((X.shape[0], self.W[1].shape[1]))\n",
    "        b_0.append(np.zeros(self.K))\n",
    "\n",
    "        update_W, update_b = W_0, b_0\n",
    "        v_W, v_b = W_0, b_0\n",
    "        m_W, m_b = W_0, b_0\n",
    "        W_original, b_original = [], []\n",
    "\n",
    "        ey = np.zeros((self.K, self.n))\n",
    "        rows = np.arange(self.n)\n",
    "        ey[Y.T, rows] = 1\n",
    "\n",
    "        for t in range(1, self.num_epochs + 1):\n",
    "\n",
    "            # wandb.log({'epoch': t})\n",
    "\n",
    "            if (self.optimizer == 'sgd'):\n",
    "                for tt in range(\n",
    "                        0,\n",
    "                    ((self.n + self.batch_size - 1) // self.batch_size)):\n",
    "                    idx = np.random.randint(self.n, size=self.batch_size)\n",
    "                    ey = np.zeros((self.K, self.batch_size))\n",
    "                    rows = np.arange(self.batch_size)\n",
    "                    ey[Y.T[idx], rows] = 1\n",
    "                    Y_pred = self.forward_prop(X.T[idx, :].T)\n",
    "\n",
    "                    self.back_prop(Y_pred, ey)\n",
    "                    dW = self.gradW[::-1]\n",
    "                    db = self.gradB[::-1]\n",
    "\n",
    "                    update_W = [self.learning_rate * u for u in dW]\n",
    "                    update_b = [self.learning_rate * u for u in db]\n",
    "                    update_W = [\n",
    "                        u + v * self.weight_decay\n",
    "                        for u, v in zip(update_W, dW)\n",
    "                    ]\n",
    "                    self.W = [u - v for u, v in zip(self.W, update_W)]\n",
    "                    self.b = [u - v for u, v in zip(self.b, update_b)]\n",
    "\n",
    "                if validate:\n",
    "                    self.validate(X.T, Y.T, X_val, Y_val)\n",
    "                continue\n",
    "\n",
    "            Y_pred = self.forward_prop(X)\n",
    "\n",
    "            if (self.optimizer == 'nesterov'):\n",
    "                W_original, b_original = self.W, self.b\n",
    "                ngamma = 0.5  # hyperparameter\n",
    "                self.W = [u - ngamma * v for u, v in zip(self.W, update_W)]\n",
    "                self.b = [u - ngamma * v for u, v in zip(self.b, update_b)]\n",
    "\n",
    "            self.back_prop(Y_pred, ey)\n",
    "\n",
    "            dW = self.gradW[::-1]\n",
    "            db = self.gradB[::-1]\n",
    "\n",
    "            if (self.optimizer == 'normal'):\n",
    "                update_W = [self.learning_rate * u for u in dW]\n",
    "                update_b = [self.learning_rate * u for u in db]\n",
    "\n",
    "            elif (self.optimizer == 'momentum'):\n",
    "                mgamma = 0.5  # hyperparameter\n",
    "                update_W = [\n",
    "                    mgamma * u + self.learning_rate * v\n",
    "                    for u, v in zip(update_W, dW)\n",
    "                ]\n",
    "                update_b = [\n",
    "                    mgamma * u + self.learning_rate * v\n",
    "                    for u, v in zip(update_b, db)\n",
    "                ]\n",
    "\n",
    "            elif (self.optimizer == 'nesterov'):\n",
    "                self.W, self.b = W_original, b_original\n",
    "                ngamma = 0.5  # hyperparameter\n",
    "                update_W = [\n",
    "                    ngamma * u + self.learning_rate * v\n",
    "                    for u, v in zip(update_W, dW)\n",
    "                ]\n",
    "                update_b = [\n",
    "                    ngamma * u + self.learning_rate * v\n",
    "                    for u, v in zip(update_b, db)\n",
    "                ]\n",
    "\n",
    "            elif (self.optimizer == 'rmsprop'):\n",
    "                rbeta = 0.9\n",
    "                epsilon = 10**-8\n",
    "                v_W = [\n",
    "                    rbeta * u + (1 - rbeta) * (v**2) for u, v in zip(v_W, dW)\n",
    "                ]\n",
    "                v_b = [\n",
    "                    rbeta * u + (1 - rbeta) * (v**2) for u, v in zip(v_b, db)\n",
    "                ]\n",
    "                update_W = [(self.learning_rate * u) / np.sqrt(v + epsilon)\n",
    "                            for u, v in zip(dW, v_W)]\n",
    "                update_b = [(self.learning_rate * u) / np.sqrt(v + epsilon)\n",
    "                            for u, v in zip(db, v_b)]\n",
    "\n",
    "            elif (self.optimizer == 'adam'):\n",
    "                abeta1 = 0.9\n",
    "                abeta2 = 0.999\n",
    "                epsilon = 10**-8\n",
    "                m_W = [abeta1 * u + (1 - abeta1) * v for u, v in zip(m_W, dW)]\n",
    "                m_b = [abeta1 * u + (1 - abeta1) * v for u, v in zip(m_b, db)]\n",
    "                v_W = [\n",
    "                    abeta2 * u + (1 - abeta2) * (v**2)\n",
    "                    for u, v in zip(v_W, dW)\n",
    "                ]\n",
    "                v_b = [\n",
    "                    abeta2 * u + (1 - abeta2) * (v**2)\n",
    "                    for u, v in zip(v_b, db)\n",
    "                ]\n",
    "                update_W = [((self.learning_rate * u) / (1 - abeta1**t)) /\n",
    "                            np.sqrt((v / (1 - abeta2**t)) + epsilon)\n",
    "                            for u, v in zip(m_W, v_W)]\n",
    "                update_b = [((self.learning_rate * u) / (1 - abeta1**t)) /\n",
    "                            np.sqrt((v / (1 - abeta2**t)) + epsilon)\n",
    "                            for u, v in zip(m_b, v_b)]\n",
    "\n",
    "            elif (self.optimizer == 'nadam'):\n",
    "                nbeta1 = 0.9\n",
    "                nbeta2 = 0.999\n",
    "                epsilon = 10**-8\n",
    "                m_W = [nbeta1 * u + (1 - nbeta1) * v for u, v in zip(m_W, dW)]\n",
    "                m_b = [nbeta1 * u + (1 - nbeta1) * v for u, v in zip(m_b, db)]\n",
    "                v_W = [\n",
    "                    nbeta2 * u + (1 - nbeta2) * (v**2)\n",
    "                    for u, v in zip(v_W, dW)\n",
    "                ]\n",
    "                v_b = [\n",
    "                    nbeta2 * u + (1 - nbeta2) * (v**2)\n",
    "                    for u, v in zip(v_b, db)\n",
    "                ]\n",
    "                update_W = [(self.learning_rate *\n",
    "                             (nbeta1 * u + (1 - nbeta1) * v)) / (1 - nbeta1**t)\n",
    "                            for u, v in zip(m_W, dW)]\n",
    "                update_W = [(u) / np.sqrt((v / (1 - nbeta2**t)) + epsilon)\n",
    "                            for u, v in zip(update_W, v_W)]\n",
    "                update_b = [(self.learning_rate *\n",
    "                             (nbeta1 * u + (1 - nbeta1) * v)) / (1 - nbeta1**t)\n",
    "                            for u, v in zip(m_b, db)]\n",
    "                update_b = [(u) / np.sqrt((v / (1 - nbeta2**t)) + epsilon)\n",
    "                            for u, v in zip(update_b, v_b)]\n",
    "\n",
    "            update_W = [\n",
    "                u + v * self.weight_decay for u, v in zip(update_W, dW)\n",
    "            ]\n",
    "\n",
    "            self.W = [u - v for u, v in zip(self.W, update_W)]\n",
    "            self.b = [u - v for u, v in zip(self.b, update_b)]\n",
    "\n",
    "            if validate:\n",
    "                self.validate(X.T, Y.T, X_val, Y_val)\n",
    "\n",
    "    def validate(self, X_tr, Y_tr, X_val, Y_val):\n",
    "        Y_pred_proba = self.predict_proba(X_tr)\n",
    "        Y_pred = Y_pred_proba.argmax(axis=1)\n",
    "        # wandb.log({'loss': self.loss_calc(Y_tr, Y_pred_proba)})\n",
    "        # wandb.log({'accuracy': self.accuracy(Y_tr, Y_pred)})\n",
    "\n",
    "        Y_pred_val_proba = self.predict_proba(X_val)\n",
    "        Y_pred_val = Y_pred_val_proba.argmax(axis=1)\n",
    "        # wandb.log({'val_loss': self.loss_calc(Y_val, Y_pred_val_proba)})\n",
    "        # wandb.log({'val_accuracy': self.accuracy(Y_val, Y_pred_val)})\n",
    "\n",
    "    def fit_validate(self, X_train, Y_train):\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X_train,\n",
    "                                                    Y_train,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=42)\n",
    "        self.n = X_tr.shape[0]\n",
    "        self.batch_size = min(self.batch_size, self.n)\n",
    "        K = np.max(Y_tr) + 1  # Y_train must have values from 0 to K - 1\n",
    "        self.layer_sizes[self.L + 1] = K\n",
    "        self.K = K\n",
    "        self.grad_desc(X_tr.T, Y_tr.T, X_val, Y_val, True)\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.n = X_train.shape[0]\n",
    "        self.batch_size = min(self.batch_size, self.n)\n",
    "        K = np.max(Y_train) + 1  # Y_train must have values from 0 to K - 1\n",
    "        self.layer_sizes[self.L + 1] = K\n",
    "        self.K = K\n",
    "        self.grad_desc(X_train.T, Y_train.T)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.forward_prop(X_test.T).T\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return (self.predict_proba(X_test).argmax(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A login cell to wandb\n",
    "\n",
    "!wandb login --relogin 2bb9d168b199eb6ac3f9f3890628908fdc41ffd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up parameters for the sweep\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'layer_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [3, 4, 5]\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['ReLU', 'sigmoid', 'tanh']\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values':\n",
    "            ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0, 0.0005, 0.5]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'num_epochs': {\n",
    "            'values': [5, 10]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.01, 0.001, 0.0001]\n",
    "        },\n",
    "        'weight_init': {\n",
    "            'values': ['random', 'Xavier']\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a sweep\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, entity = '0x2e4', project = 'cs6910-a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A training function for the sweep\n",
    "\n",
    "def train():\n",
    "    def_params = {\n",
    "        'layer_size': 32,\n",
    "        'num_layers': 4,\n",
    "        'activation': 'ReLU',\n",
    "        'optimizer': 'adam',\n",
    "        'weight_decay': 0.0005,\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 5,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_init': 'random'\n",
    "    }\n",
    "    run = wandb.init(config=def_params, reinit=True)\n",
    "    config = wandb.config\n",
    "    wandb.run.name = str(config.layer_size) + '_' + str(config.num_layers) + '_' + config.activation[0] + '_' + config.optimizer[:2] + '_' + str(config.weight_decay) + '_' + str(config.batch_size) + '_' + str(config.num_epochs) + '_' + str(config.learning_rate) + '_' + config.weight_init[0]\n",
    "    wandb.run.save()\n",
    "    model = FNNClassifier(**config)\n",
    "    model.fit_validate(X_train_new, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a sweep agent\n",
    "\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating best fit FNN for Fashion - MNIST\n",
    "\n",
    "best_model_entropy = FNNClassifier(64, 5, 'ReLU', 'sgd', 0, 16, 0.0001, 10, 'Xavier')\n",
    "best_model_entropy.fit(X_train_new, Y_train)\n",
    "Y_pred = best_model_entropy.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a confusion matrix on test data\n",
    "\n",
    "conf_mat = np.zeros((10, 10), dtype = np.int64)\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    conf_mat[Y_pred[i]][Y_test[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging the confusion matrix \n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "v = sn.heatmap(conf_mat, cmap = 'YlGnBu', annot = True, fmt = 'd')\n",
    "wandb.log({'Confusion matrix' : wandb.Image(v)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
