{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing GPU availability\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return a CNN model based on input hyperparameters\n",
    "\n",
    "def createCNN(n_filters,\n",
    "              filter_size,\n",
    "              n_dense,\n",
    "              num_conv_layers,\n",
    "              input_shape,\n",
    "              activation,\n",
    "              n_output,\n",
    "              filter_size_decay=1,\n",
    "              dropout_conv=0.0,\n",
    "              dropout_dense=0.0,\n",
    "              batch_normalize=False,\n",
    "              data_augment=False):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    if data_augment:\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            layers.experimental.preprocessing.RandomFlip(\n",
    "                \"horizontal_and_vertical\"),\n",
    "            layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "        ])\n",
    "        model.add(data_augmentation)\n",
    "\n",
    "    for ii in range(0, num_conv_layers):\n",
    "        if ii == 0:\n",
    "            if batch_normalize:\n",
    "                model.add(layers.BatchNormalization(input_shape=input_shape))\n",
    "            model.add(\n",
    "                layers.Conv2D(n_filters, (filter_size, filter_size),\n",
    "                              activation=activation,\n",
    "                              input_shape=input_shape))\n",
    "        else:\n",
    "            if batch_normalize:\n",
    "                model.add(layers.BatchNormalization())\n",
    "            model.add(\n",
    "                layers.Conv2D(n_filters, (filter_size, filter_size),\n",
    "                              activation=activation))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        if dropout_conv != 0.0:\n",
    "            model.add(layers.Dropout(dropout_conv))\n",
    "\n",
    "        n_filters = int(n_filters * filter_size_decay)\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(n_dense, activation=activation))\n",
    "    if dropout_dense != 0.0:\n",
    "        model.add(layers.Dropout(dropout_dense))\n",
    "    model.add(layers.Dense(n_output))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up input flows\n",
    "\n",
    "dataset_path = \"../../nature_12K/inaturalist_12K\"\n",
    "train_path = dataset_path + \"/train\"\n",
    "test_path = dataset_path + \"/val\"\n",
    "max_shape = (256, 256, 3)\n",
    "\n",
    "data_train = ImageDataGenerator(validation_split=0.1)  #, rescale = 1. / 255)\n",
    "\n",
    "data_test = ImageDataGenerator()  #rescale = 1. / 255)\n",
    "\n",
    "full_train_generator = ImageDataGenerator().flow_from_directory(\n",
    "    train_path, class_mode='categorical')\n",
    "\n",
    "train_generator = data_train.flow_from_directory(train_path,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 subset='training')\n",
    "\n",
    "validation_generator = data_train.flow_from_directory(train_path,\n",
    "                                                      class_mode='categorical',\n",
    "                                                      subset='validation')\n",
    "\n",
    "test_generator = data_test.flow_from_directory(test_path,\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A training function to train a model\n",
    "\n",
    "def train(model, optimizer, loss_fn, epochs=10):\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "    model.fit(train_generator,\n",
    "              epochs=epochs,\n",
    "              validation_data=validation_generator,\n",
    "              callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up hyperparameters and configurations for the sweep\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        \"n_filters\": {\n",
    "            'values': [32, 64]\n",
    "        },\n",
    "        \"filter_size_decay\": {\n",
    "            'values': [0.5, 1, 1.15]\n",
    "        },\n",
    "        \"data_augment\": {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            'values': [0.0, 0.2, 0.4]\n",
    "        },\n",
    "        \"batch_normalize\": {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        \"filter_size\": {\n",
    "            'values': [3, 5]\n",
    "        },\n",
    "        \"n_dense\": {\n",
    "            'values': [50, 100]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sweep\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, entity = '0x2e4', project = 'cs6910-a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for the wandb sweep\n",
    "\n",
    "def run():\n",
    "    default_config = {\n",
    "        \"n_filters\": 32,\n",
    "        \"filter_size_decay\": 1,\n",
    "        \"data_augment\": False,\n",
    "        \"dropout\": 0.0,\n",
    "        \"batch_normalize\": False,\n",
    "        \"filter_size\": 3,\n",
    "        \"n_dense\": 50\n",
    "    }\n",
    "\n",
    "    run = wandb.init(project='cs6910-a2', config=default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    model = createCNN(n_filters=config.n_filters,\n",
    "                      filter_size=config.filter_size,\n",
    "                      n_dense=config.n_dense,\n",
    "                      num_conv_layers=5,\n",
    "                      input_shape=max_shape,\n",
    "                      activation='relu',\n",
    "                      n_output=10,\n",
    "                      filter_size_decay=config.filter_size_decay,\n",
    "                      dropout_conv=config.dropout / 3,\n",
    "                      dropout_dense=config.dropout,\n",
    "                      batch_normalize=config.batch_normalize,\n",
    "                      data_augment=config.data_augment)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Nadam()\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    train(model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a wandb agent to run the sweep\n",
    "\n",
    "wandb.agent(sweep_id, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the best model\n",
    "\n",
    "best_model = createCNN(n_filters=32,\n",
    "                       filter_size=3,\n",
    "                       n_dense=100,\n",
    "                       num_conv_layers=5,\n",
    "                       input_shape=max_shape,\n",
    "                       activation='relu',\n",
    "                       n_output=10,\n",
    "                       filter_size_decay=1.15,\n",
    "                       data_augment=True)\n",
    "best_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "best_model.fit(full_train_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting results based on the best model\n",
    "\n",
    "y_pred_l = best_model.predict(test_generator)\n",
    "y_pred = y_pred_l.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing accuracy\n",
    "\n",
    "y_true = np.array([np.zeros(200) + ii for ii in range(0, 10)]).flatten()\n",
    "(y_pred == y_true).sum() / 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sample plots with their predictions and true classes\n",
    "\n",
    "pyplot.figure(figsize=[50, 100])\n",
    "all_classes = list(test_generator.class_indices.keys())\n",
    "\n",
    "ax = pyplot.subplot(11, 3, 1)\n",
    "ax1 = pyplot.subplot(11, 3, 2)\n",
    "ax2 = pyplot.subplot(11, 3, 3)\n",
    "ax.axis('off')\n",
    "ax.text(0.3, 0.5, \"Sample Image\", fontsize=70)\n",
    "ax1.axis('off')\n",
    "ax1.text(0.3, 0.5, \"Prediction\", fontsize=70)\n",
    "ax2.axis('off')\n",
    "ax2.text(0.3, 0.5, \"True Class\", fontsize=70)\n",
    "\n",
    "for some in os.listdir(test_path):\n",
    "    idx = test_generator.class_indices[some]\n",
    "    new_path = test_path + \"/\" + some\n",
    "    img_path = new_path + \"/\" + os.listdir(new_path)[0]\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path,\n",
    "                                                target_size=(max_shape[0],\n",
    "                                                             max_shape[1]))\n",
    "    img_np = np.asarray(img)\n",
    "    ax = pyplot.subplot(11, 3, 3 * idx + 4)\n",
    "    ax1 = pyplot.subplot(11, 3, 3 * idx + 5)\n",
    "    ax2 = pyplot.subplot(11, 3, 3 * idx + 6)\n",
    "    ax.imshow(img_np)\n",
    "    ax1.axis('off')\n",
    "    ax1.text(0.3, 0.5, all_classes[y_pred[200 * idx]], fontsize=70)\n",
    "    ax2.axis('off')\n",
    "    ax2.text(0.3, 0.5, some, fontsize=70)\n",
    "\n",
    "wandb.init(project='cs6910-a2')\n",
    "wandb.log({'Sample Predictions': pyplot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking out first layer of filters from the best model\n",
    "\n",
    "cat_path = train_path + \"/Mammalia/26b7e18a07cb0fb7576b5019c2ea1577.jpg\"\n",
    "cat_np = np.asarray(\n",
    "    tf.keras.preprocessing.image.load_img(cat_path,\n",
    "                                          target_size=(max_shape[0],\n",
    "                                                       max_shape[1])))\n",
    "cat_np = np.expand_dims(cat_np, axis=0)\n",
    "\n",
    "filter_vis = models.Sequential()\n",
    "filter_vis.add(best_model.layers[1])\n",
    "result = filter_vis.predict(cat_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the filters on an image\n",
    "\n",
    "pyplot.figure(figsize=[50, 100])\n",
    "\n",
    "for ii in range(8):\n",
    "    for jj in range(4):\n",
    "        ax = pyplot.subplot(8, 4, 4 * ii + jj + 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        pyplot.imshow(result[0, :, :, ii * 4 + jj])\n",
    "\n",
    "wandb.init(project='cs6910-a2')\n",
    "wandb.log({'Filters Visualization': pyplot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up guided backpropogation\n",
    "\n",
    "@tf.custom_gradient\n",
    "def guidedRelu(x):\n",
    "    def grad(dy):\n",
    "        return tf.cast(dy > 0, \"float32\") * tf.cast(x > 0, \"float32\") * dy\n",
    "\n",
    "    return tf.nn.relu(x), grad\n",
    "\n",
    "\n",
    "guided_bp_model = [\n",
    "    tf.keras.models.Model(\n",
    "        inputs=[best_model.inputs],\n",
    "        outputs=[best_model.get_layer(\"conv2d_4\").output[:, :, :, ii]])\n",
    "    for ii in range(0, 10)\n",
    "]\n",
    "\n",
    "layers = [[\n",
    "    each for each in guided_bp_model[ii].layers[1:]\n",
    "    if hasattr(each, 'activation')\n",
    "] for ii in range(0, 10)]\n",
    "for every in layers:\n",
    "    for layer in every:\n",
    "        if layer.activation == tf.keras.activations.relu:\n",
    "            layer.activation = guidedRelu\n",
    "\n",
    "grads = []\n",
    "for ii in range(0, 10):\n",
    "    with tf.GradientTape() as g:\n",
    "        inputs = tf.cast(cat_np, tf.float32)\n",
    "        g.watch(inputs)\n",
    "        outputs = guided_bp_model[ii](inputs)\n",
    "    grads.append(np.asarray(g.gradient(outputs, inputs)[0]).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprocessing the gradient for better image visualization\n",
    "\n",
    "for ii in range(0, 10):\n",
    "    grads[ii] -= grads[ii].mean()\n",
    "    grads[ii] /= (grads[ii].std() + tf.keras.backend.epsilon())\n",
    "    grads[ii] = (grads[ii] * 0.25) + 0.5\n",
    "    grads[ii] = np.clip(grads[ii], 0, 1) * 255\n",
    "    grads[ii] = grads[ii].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and logging guided backpropogation\n",
    "\n",
    "pyplot.figure(figsize=[50, 100])\n",
    "for ii in range(0, 10):\n",
    "    ax = pyplot.subplot(10, 1, ii + 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    pyplot.imshow(np.flip(np.array(grads[ii]),-1))\n",
    "\n",
    "wandb.init(project = 'cs6910-a2')\n",
    "wandb.log({'Guided Backpropagation' : pyplot})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
