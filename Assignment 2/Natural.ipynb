{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "organized-feedback",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-8a5c28967f04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from matplotlib import pyplot, image\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vietnamese-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return a CNN model based on input hyperparameters\n",
    "def createCNN(n_filters, filter_size, n_dense, num_conv_layers, input_shape, activation, n_output, filter_size_decay = 1, dropout_conv = 0.0, dropout_dense = 0.0, batch_normalize = False, data_augment = False):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    if data_augment:\n",
    "        data_augmentation = tf.keras.Sequential([ \n",
    "                            layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                            layers.experimental.preprocessing.RandomRotation(0.2)])\n",
    "        model.add(data_augmentation)\n",
    "    \n",
    "    \n",
    "    for ii in range(0, num_conv_layers): # Number of convolutional layers is num_conv_layers\n",
    "        if ii == 0:\n",
    "            if batch_normalize:\n",
    "                model.add(layers.BatchNormalization(input_shape=input_shape))\n",
    "            model.add(layers.Conv2D(n_filters, (filter_size, filter_size), activation=activation, input_shape=input_shape))\n",
    "        else:\n",
    "            if batch_normalize:\n",
    "                model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Conv2D(n_filters, (filter_size, filter_size), activation=activation))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        if dropout_conv != 0.0:\n",
    "            model.add(layers.Dropout(dropout_conv))\n",
    "    \n",
    "        n_filters = int(n_filters * filter_size_decay)\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(n_dense, activation=activation))\n",
    "    if dropout_dense != 0.0:\n",
    "        model.add(layers.Dropout(dropout_dense))\n",
    "    model.add(layers.Dense(n_output))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../nature_12K/inaturalist_12K\"\n",
    "train_path = dataset_path + \"/train\"\n",
    "test_path = dataset_path + \"/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "max_shape = (32, 32, 3)\n",
    "\n",
    "data_train = ImageDataGenerator(validation_split = 0.1)\n",
    "\n",
    "data_test = ImageDataGenerator()\n",
    "\n",
    "train_generator = data_train.flow(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        subset = 'training')\n",
    "\n",
    "validation_generator = data_train.flow(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        subset = 'validation')\n",
    "\n",
    "test_generator = data_train.flow(\n",
    "        test_images,\n",
    "        test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = my_cnn(16, 3, 64, (32, 32, 3), 'relu', 10)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "realistic-notice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9999 images belonging to 10 classes.\n",
      "Found 9000 images belonging to 10 classes.\n",
      "Found 999 images belonging to 10 classes.\n",
      "Found 2000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../../nature_12K/inaturalist_12K\"\n",
    "train_path = dataset_path + \"/train\"\n",
    "test_path = dataset_path + \"/val\"\n",
    "max_shape = (256, 256, 3)\n",
    "\n",
    "data_train = ImageDataGenerator(validation_split = 0.1)#, rescale = 1. / 255)\n",
    "\n",
    "data_test = ImageDataGenerator()#rescale = 1. / 255)\n",
    "\n",
    "full_train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_path,\n",
    "        class_mode='categorical')\n",
    "\n",
    "train_generator = data_train.flow_from_directory(\n",
    "        train_path,\n",
    "        class_mode='categorical',\n",
    "        subset = 'training')\n",
    "\n",
    "validation_generator = data_train.flow_from_directory(\n",
    "        train_path,\n",
    "        class_mode='categorical',\n",
    "        subset = 'validation')\n",
    "\n",
    "test_generator = data_test.flow_from_directory(\n",
    "        test_path,\n",
    "        class_mode='categorical',\n",
    "        shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          epochs=10):\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics = ['accuracy'])\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)#, callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    }, 'parameters' : {\n",
    "              \"n_filters\": {'values': [32, 64]},\n",
    "              \"filter_size_decay\" : {'values':[0.5, 1, 1.15]},\n",
    "              \"data_augment\": {'values':[True, False]},\n",
    "              \"dropout\" : {'values':[0.0, 0.2, 0.4]},\n",
    "              \"batch_normalize\" : {'values':[True, False]},\n",
    "              \"filter_size\" : {'values':[3, 5]},\n",
    "              \"n_dense\" : {'values':[50, 100]}\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, entity = '0x2e4', project = 'cs6910-a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    default_config = {\n",
    "              \"n_filters\": 32,\n",
    "              \"filter_size_decay\" : 1,\n",
    "              \"data_augment\": False,\n",
    "              \"dropout\" : 0.0,\n",
    "              \"batch_normalize\" : False,\n",
    "              \"filter_size\" : 3,\n",
    "              \"n_dense\" : 50\n",
    "           }\n",
    "\n",
    "    run = wandb.init(project='cs6910-a2', config=default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # initialize model\n",
    "    model = createCNN(n_filters = config.n_filters, filter_size = config.filter_size, n_dense = config.n_dense, num_conv_layers = 5, input_shape = max_shape, activation = 'relu', n_output = 10, filter_size_decay=config.filter_size_decay, dropout_conv = config.dropout / 3, dropout_dense = config.dropout, batch_normalize=config.batch_normalize, data_augment=config.data_augment)\n",
    "\n",
    "    # Instantiate an optimizer to train the model.\n",
    "    optimizer = tf.keras.optimizers.Nadam()\n",
    "    # Instantiate a loss function.\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    train(model,\n",
    "      optimizer,\n",
    "      loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "a = [[[0 for k in range(4)] for i in range(4)] for j in range(15)]\n",
    "b = [0 for k in range(15)]\n",
    "\n",
    "for i in range(0, 15):\n",
    "    for j in range(0, 4):\n",
    "        for k in range(0, 4):\n",
    "            a[i][j][k] = random.randint(0, 15)\n",
    "    b[i] = random.randint(1, 3)\n",
    "\n",
    "f = open('train.txt', 'w')\n",
    "f.write(str(a))\n",
    "f.close()\n",
    "f = open('test.txt', 'w')\n",
    "f.write(str(b))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "f = open('train.txt', 'r')\n",
    "train_s = np.asarray(ast.literal_eval(f.read()))\n",
    "print(train_s)\n",
    "f.close()\n",
    "\n",
    "f = open('test.txt', 'r')\n",
    "test_s = np.asarray(ast.literal_eval(f.read()))\n",
    "print(test_s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model = createCNN(10, 2, 2, 3, (32, 32, 3), 'relu', 10)\n",
    "testing_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model.compile(optimizer='nadam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = testing_model.fit(X_train, y_train, epochs=10, \n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "immediate-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createCNN(n_filters = 32, filter_size = 3, n_dense = 100, num_conv_layers = 5, input_shape = max_shape, activation = 'relu', n_output = 10, batch_normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "likely-ribbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 256, 256, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 127, 127, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 125, 125, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 62, 62, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 60, 60, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 154,722\n",
      "Trainable params: 154,460\n",
      "Non-trainable params: 262\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train(model,\n",
    "  optimizer,\n",
    "  loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "seeing-ranch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 159s 507ms/step - loss: 2.7122 - accuracy: 0.1682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f7a996ad90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = createCNN(n_filters = 32, filter_size = 3, n_dense = 100, num_conv_layers = 5, input_shape = max_shape, activation = 'relu', n_output = 10, filter_size_decay = 1.15, data_augment = True)\n",
    "best_model.compile(optimizer = tf.keras.optimizers.Nadam(), loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics = ['accuracy'])\n",
    "best_model.fit(full_train_generator, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "sealed-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_l = best_model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "harmful-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred_l.argmax(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "voluntary-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([np.zeros(200) + ii for ii in range(0, 10)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "periodic-favorite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1965"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred == y_true).sum() / 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "reasonable-worker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: 0x2e4 (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">generous-eon-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/0x2e4/DeepLearning-Assignment%202\" target=\"_blank\">https://wandb.ai/0x2e4/DeepLearning-Assignment%202</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/0x2e4/DeepLearning-Assignment%202/runs/1yq2or9d\" target=\"_blank\">https://wandb.ai/0x2e4/DeepLearning-Assignment%202/runs/1yq2or9d</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\Assignment 2\\wandb\\run-20210409_233808-1yq2or9d</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init()\n",
    "for some in os.listdir(test_path):\n",
    "    idx = test_generator.class_indices[some]\n",
    "    new_path = test_path + \"/\" + some\n",
    "    img_path = new_path + \"/\" + os.listdir(new_path)[0]\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "    wandb.log({'img': wandb.Image(img, caption='Set ' + str(idx)),'class' : idx,'prediction':y_pred[200 * idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-uniform",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
