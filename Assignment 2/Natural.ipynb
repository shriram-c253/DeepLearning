{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from matplotlib import image\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return a CNN model based on input hyperparameters\n",
    "def createCNN(n_filters, filter_size, n_dense, num_conv_layers, input_shape, activation, n_output, filter_size_decay = 1, dropout_conv = 0.0, dropout_dense = 0.0, batch_normalize = False, data_augment = False):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    if data_augment:\n",
    "        data_augmentation = tf.keras.Sequential([ \n",
    "                            layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                            layers.experimental.preprocessing.RandomRotation(0.2)])\n",
    "        model.add(data_augmentation)\n",
    "    \n",
    "    if batch_normalize:\n",
    "        model.add(layers.BatchNormalization(input_size=input_size))\n",
    "    for ii in range(0, num_conv_layers): # Number of convolutional layers is num_conv_layers\n",
    "        if ii == 0:\n",
    "            model.add(layers.Conv2D(n_filters, (filter_size, filter_size), activation=activation, input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(layers.Conv2D(n_filters, (filter_size, filter_size), activation=activation))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        if dropout_conv != 0.0:\n",
    "            model.add(layers.Dropout(dropout_conv))\n",
    "    \n",
    "        n_filters *= filter_size_decay\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(n_dense, activation=activation))\n",
    "    if dropout_dense != 0.0:\n",
    "        model.add(layers.Dropout(dropout_dense))\n",
    "    model.add(layers.Dense(n_output))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../nature_12K/inaturalist_12K\"\n",
    "train_path = dataset_path + \"/train\"\n",
    "test_path = dataset_path + \"/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "max_shape = (32, 32, 3)\n",
    "\n",
    "train_size = int(train_labels.shape[0] * 0.9)\n",
    "val_size = int(train_labels.shape[0] * 0.1)\n",
    "test_size = test_labels.shape[0]\n",
    "\n",
    "data_train = ImageDataGenerator(validation_split = 0.1)\n",
    "\n",
    "data_test = ImageDataGenerator()\n",
    "\n",
    "train_generator = data_train.flow(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        batch_size=train_size,\n",
    "        subset = 'training')\n",
    "\n",
    "validation_generator = data_train.flow(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        batch_size=val_size,\n",
    "        subset = 'validation')\n",
    "\n",
    "test_generator = data_train.flow(\n",
    "        test_images,\n",
    "        test_labels,\n",
    "        batch_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = my_cnn(16, 3, 64, (32, 32, 3), 'relu', 10)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../nature_12K/inaturalist_12K\"\n",
    "train_path = dataset_path + \"/train\"\n",
    "test_path = dataset_path + \"/val\"\n",
    "max_shape = (300, 300, 3)\n",
    "\n",
    "data_train = ImageDataGenerator(validation_split = 0.1)#, rescale = 1. / 255)\n",
    "\n",
    "data_test = ImageDataGenerator()#rescale = 1. / 255)\n",
    "\n",
    "train_generator = data_train.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(max_shape[0], max_shape[1]),\n",
    "        batch_size=64,classes=None,\n",
    "        class_mode='categorical',\n",
    "        subset = 'training')\n",
    "\n",
    "validation_generator = data_train.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(max_shape[0], max_shape[1]),\n",
    "        batch_size=64,classes=None,\n",
    "        class_mode='categorical',\n",
    "        subset = 'validation')\n",
    "\n",
    "test_generator = data_test.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(max_shape[0], max_shape[1]),\n",
    "        batch_size=64,classes=None,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          epochs=10):\n",
    "    print('hu')\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics = ['accuracy'])\n",
    "    print('ho')\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)#, callbacks=[WandbCallback()])\n",
    "    print('ha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    }, 'parameters' : {\n",
    "              \"n_filters\": {'values': [10]},\n",
    "              \"filter_factor\" : {'values':[1]},\n",
    "              \"data_augment\": {'values':[False]},\n",
    "              \"dropout\" : {'values':[0.0]},\n",
    "              \"batch_normalize\" : {'values':[False]}\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, entity = '0x2e4', project = 'cs6910-a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    default_config = {\n",
    "              \"n_filters\": 10,\n",
    "              \"filter_factor\" : 1,\n",
    "              \"data_augment\": False,\n",
    "              \"dropout\" : 0.0,\n",
    "              \"batch_normalize\" : False\n",
    "           }\n",
    "\n",
    "    run = wandb.init(project='cs6910-a2', config=default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # initialize model\n",
    "    model = createCNN(n_filters = config.n_filters, filter_size = 2, n_dense = 1, num_conv_layers = 3, input_shape = max_shape, activation = 'relu', n_output = 10, dropout_conv = config.dropout / 3, dropout_dense = config.dropout, batch_normalize=config.batch_normalize, data_augment=config.data_augment)\n",
    "\n",
    "    # Instantiate an optimizer to train the model.\n",
    "    optimizer = tf.keras.optimizers.Nadam()\n",
    "    # Instantiate a loss function.\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(logits=True)\n",
    "\n",
    "    train(model,\n",
    "      optimizer,\n",
    "      loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "a = [[[0 for k in range(4)] for i in range(4)] for j in range(15)]\n",
    "b = [0 for k in range(15)]\n",
    "\n",
    "for i in range(0, 15):\n",
    "    for j in range(0, 4):\n",
    "        for k in range(0, 4):\n",
    "            a[i][j][k] = random.randint(0, 15)\n",
    "    b[i] = random.randint(1, 3)\n",
    "\n",
    "f = open('train.txt', 'w')\n",
    "f.write(str(a))\n",
    "f.close()\n",
    "f = open('test.txt', 'w')\n",
    "f.write(str(b))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "f = open('train.txt', 'r')\n",
    "train_s = np.asarray(ast.literal_eval(f.read()))\n",
    "print(train_s)\n",
    "f.close()\n",
    "\n",
    "f = open('test.txt', 'r')\n",
    "test_s = np.asarray(ast.literal_eval(f.read()))\n",
    "print(test_s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model = createCNN(10, 2, 2, 3, (32, 32, 3), 'relu', 10)\n",
    "testing_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model.compile(optimizer='nadam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = testing_model.fit(X_train, y_train, epochs=10, \n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createCNN(n_filters = 32, filter_size = 3, n_dense = 100, num_conv_layers = 5, input_shape = (300, 300, 3), activation = 'relu', n_output = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train(model,\n",
    "  optimizer,\n",
    "  loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
