{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mounted-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from matplotlib import image\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "optical-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return a CNN model based on input hyperparameters\n",
    "def createCNN(n_filters, filter_size, n_dense, num_conv_layers, input_shape, activation, n_output, filter_size_decay = 1, dropout_conv = 0.0, dropout_dense = 0.0, batch_normalize = False, data_augment = False):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    if data_augment:\n",
    "        data_augmentation = tf.keras.Sequential([ \n",
    "                            layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                            layers.experimental.preprocessing.RandomRotation(0.2)])\n",
    "        model.add(data_augmentation)\n",
    "    \n",
    "    if batch_normalize:\n",
    "        model.add(layers.BatchNormalization(input_size=input_size))\n",
    "    for ii in range(0, num_conv_layers): # Number of convolutional layers is num_conv_layers\n",
    "        if ii == 0:\n",
    "            model.add(layers.Conv2D(n_filters, (filter_size, filter_size), activation=activation, input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(layers.Conv2D(n_filters, (filter_size, filter_size), activation=activation))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        if dropout_conv != 0.0:\n",
    "            model.add(layers.Dropout(dropout_conv))\n",
    "    \n",
    "        n_filters = int(n_filters * filter_size_decay)\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(n_dense, activation=activation))\n",
    "    if dropout_dense != 0.0:\n",
    "        model.add(layers.Dropout(dropout_dense))\n",
    "    model.add(layers.Dense(n_output))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../nature_12K/inaturalist_12K\"\n",
    "train_path = dataset_path + \"/train\"\n",
    "test_path = dataset_path + \"/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "max_shape = (32, 32, 3)\n",
    "\n",
    "data_train = ImageDataGenerator(validation_split = 0.1)\n",
    "\n",
    "data_test = ImageDataGenerator()\n",
    "\n",
    "train_generator = data_train.flow(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        subset = 'training')\n",
    "\n",
    "validation_generator = data_train.flow(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        subset = 'validation')\n",
    "\n",
    "test_generator = data_train.flow(\n",
    "        test_images,\n",
    "        test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = my_cnn(16, 3, 64, (32, 32, 3), 'relu', 10)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "matched-seminar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9000 images belonging to 10 classes.\n",
      "Found 999 images belonging to 10 classes.\n",
      "Found 2000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../../nature_12K/inaturalist_12K\"\n",
    "train_path = dataset_path + \"/train\"\n",
    "test_path = dataset_path + \"/val\"\n",
    "max_shape = (256, 256, 3)\n",
    "\n",
    "data_train = ImageDataGenerator(validation_split = 0.1)#, rescale = 1. / 255)\n",
    "\n",
    "data_test = ImageDataGenerator()#rescale = 1. / 255)\n",
    "\n",
    "train_generator = data_train.flow_from_directory(\n",
    "        train_path,\n",
    "        class_mode='categorical',\n",
    "        subset = 'training')\n",
    "\n",
    "validation_generator = data_train.flow_from_directory(\n",
    "        train_path,\n",
    "        class_mode='categorical',\n",
    "        subset = 'validation')\n",
    "\n",
    "test_generator = data_test.flow_from_directory(\n",
    "        test_path,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convinced-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          epochs=10):\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics = ['accuracy'])\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)#, callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    }, 'parameters' : {\n",
    "              \"n_filters\": {'values': [32, 64]},\n",
    "              \"filter_size_decay\" : {'values':[0.5, 1, 1.15]},\n",
    "              \"data_augment\": {'values':[True, False]},\n",
    "              \"dropout\" : {'values':[0.0, 0.2, 0.4]},\n",
    "              \"batch_normalize\" : {'values':[True, False]},\n",
    "              \"filter_size\" : {'values':[3, 5]},\n",
    "              \"n_dense\" : {'values':[50, 100]}\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, entity = '0x2e4', project = 'cs6910-a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    default_config = {\n",
    "              \"n_filters\": 32,\n",
    "              \"filter_size_decay\" : 1,\n",
    "              \"data_augment\": False,\n",
    "              \"dropout\" : 0.0,\n",
    "              \"batch_normalize\" : False,\n",
    "              \"filter_size\" : 3,\n",
    "              \"n_dense\" : 50\n",
    "           }\n",
    "\n",
    "    run = wandb.init(project='cs6910-a2', config=default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # initialize model\n",
    "    model = createCNN(n_filters = config.n_filters, filter_size = config.filter_size, n_dense = config.n_dense, num_conv_layers = 5, input_shape = max_shape, activation = 'relu', n_output = 10, filter_size_decay=config.filter_size_decay, dropout_conv = config.dropout / 3, dropout_dense = config.dropout, batch_normalize=config.batch_normalize, data_augment=config.data_augment)\n",
    "\n",
    "    # Instantiate an optimizer to train the model.\n",
    "    optimizer = tf.keras.optimizers.Nadam()\n",
    "    # Instantiate a loss function.\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    train(model,\n",
    "      optimizer,\n",
    "      loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "a = [[[0 for k in range(4)] for i in range(4)] for j in range(15)]\n",
    "b = [0 for k in range(15)]\n",
    "\n",
    "for i in range(0, 15):\n",
    "    for j in range(0, 4):\n",
    "        for k in range(0, 4):\n",
    "            a[i][j][k] = random.randint(0, 15)\n",
    "    b[i] = random.randint(1, 3)\n",
    "\n",
    "f = open('train.txt', 'w')\n",
    "f.write(str(a))\n",
    "f.close()\n",
    "f = open('test.txt', 'w')\n",
    "f.write(str(b))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "f = open('train.txt', 'r')\n",
    "train_s = np.asarray(ast.literal_eval(f.read()))\n",
    "print(train_s)\n",
    "f.close()\n",
    "\n",
    "f = open('test.txt', 'r')\n",
    "test_s = np.asarray(ast.literal_eval(f.read()))\n",
    "print(test_s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model = createCNN(10, 2, 2, 3, (32, 32, 3), 'relu', 10)\n",
    "testing_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model.compile(optimizer='nadam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = testing_model.fit(X_train, y_train, epochs=10, \n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "imposed-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createCNN(n_filters = 32, filter_size = 3, n_dense = 100, num_conv_layers = 5, input_shape = max_shape, activation = 'relu', n_output = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "buried-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 125, 125, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 154,198\n",
      "Trainable params: 154,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-documentary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  2/282 [..............................] - ETA: 47s - loss: 19.9166 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.2405s). Check your callbacks.\n",
      " 46/282 [===>..........................] - ETA: 4:12 - loss: 3.7756 - accuracy: 0.1333"
     ]
    }
   ],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train(model,\n",
    "  optimizer,\n",
    "  loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-delivery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
