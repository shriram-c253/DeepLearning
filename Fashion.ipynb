{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from keras.datasets import fashion_mnist\n",
    "#import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train) , (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = set()\n",
    "fig, ax = plt.subplots(5, 2, figsize = (25, 25))\n",
    "fig.tight_layout()\n",
    "for x, y in zip(X_train, Y_train):\n",
    "    if y not in done:\n",
    "        done.add(y)\n",
    "        ax[y % 5, y // 5].imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNClassifier:\n",
    "    def __init__(self, layer_size, num_layers, activation='ReLU', optimizer='adam', weight_decay=0.0001, batch_size=200, learning_rate=0.001, num_epochs=200, weight_init='Xavier'):\n",
    "        self.activation = activation # 'identity', 'logistic', 'tanh', 'relu'\n",
    "        self.optimizer = optimizer # 'normal', 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'\n",
    "        self.weight_decay = weight_decay # L2 regularization hyperparameter\n",
    "        self.batch_size = batch_size # Batch size\n",
    "        self.learning_rate = learning_rate # Learning Rate\n",
    "        self.num_epochs = num_epochs # Number of epochs\n",
    "        self.weight_init = weight_init # 'random', 'xavier'\n",
    "        self.n = 100\n",
    "        self.K = 10\n",
    "        self.L = num_layers\n",
    "        self.N = layer_size\n",
    "        self.layer_sizes = np.zeros((num_layers + 2))\n",
    "        self.layer_sizes[1 : num_layers + 1] = layer_size\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.a = []\n",
    "        self.h = []\n",
    "        wandb.log({'layer_sizes': layer_size})\n",
    "        wandb.log({'num_layers': num_layers})\n",
    "        wandb.log({'activation': activation})\n",
    "        wandb.log({'optimizer': optimizer})\n",
    "        wandb.log({'weight_decay': weight_decay})\n",
    "        wandb.log({'batch_size': batch_size})\n",
    "        wandb.log({'learning_rate': learning_rate})\n",
    "        wandb.log({'weight_init': weight_init})\n",
    "    \n",
    "    def act(self, z):\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif(self.activation == 'tanh'):\n",
    "            return np.tanh(z)\n",
    "        elif(self.activation == 'ReLU'):\n",
    "            return np.maximum(z, np.zeros(z.shape)) \n",
    "    \n",
    "    def deriv_act(self, z):\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            return 1 / (2 + np.exp(-z) + np.exp(z))\n",
    "        elif(self.activation == 'tanh'):\n",
    "            return np.cosh(z) ** -2\n",
    "        elif(self.activation == 'ReLU'):\n",
    "            return np.maximum(np.sign(z), np.zeros(z.shape))\n",
    "    \n",
    "    def oact(self, z):\n",
    "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        self.a = [ np.zeros((1, 1))]\n",
    "        self.h = [X]\n",
    "        \n",
    "        for i in range(1, self.L + 1):\n",
    "            self.a.append( (self.b[i].T + (self.W[i].T @ self.h[i - 1]).T ).T)\n",
    "            self.h.append(self.act(self.a[i]))\n",
    "        self.h[-1] = self.oact(self.a[-1])\n",
    "        return self.h[-1]\n",
    "    \n",
    "    def back_prop(self, Y_pred, ey):\n",
    "        self.gradW, self.gradB, self.grada = [], [], []\n",
    "        self.gradh = [ np.zeros((1, 1))]\n",
    "        self.grada.append(-(ey - Y_pred))\n",
    "        for i in range (self.L, 0, -1):\n",
    "            self.gradW.append((self.grada[self.L - i] @ self.h[i - 1].T).T)\n",
    "            self.gradB.append(self.grada[self.L - i].sum(axis=1))\n",
    "            self.gradh.append(self.W[i] @ self.grada[self.L - i])\n",
    "            self.grada.append( self.gradh[self.L - i + 1] * self.deriv_act(self.a[i - 1]))\n",
    "        \n",
    "        self.gradW.append(np.zeros((self.N, self.N)))\n",
    "        self.gradB.append(np.zeros(self.N))\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def wb_init(self, num):\n",
    "        L = 10\n",
    "        if(self.weight_init == 'random'):\n",
    "            for i in range(self.L):\n",
    "                self.W.append(np.random.randn(self.N, self.N))\n",
    "                self.b.append(np.random.randn(self.N))\n",
    "            self.W.append(np.random.randn(self.N, self.K))\n",
    "            self.W[1] = np.random.randn(num, self.W[1].shape[1])\n",
    "            self.b.append(np.random.randn(self.K))\n",
    "            \n",
    "        elif (self.weight_init == 'Xavier'):\n",
    "            for i in range(self.L):\n",
    "                self.W.append(np.random.normal(0, np.sqrt(1 / self.N), (self.N, self.N)))\n",
    "                self.b.append(np.random.normal(0, np.sqrt(1 / self.N), self.N))\n",
    "            self.W.append(np.random.normal(0, np.sqrt(2 / (self.N + self.K)), (self.N, self.K)))\n",
    "            self.W[1] = np.random.normal(0, np.sqrt(2 / (num + self.W[1].shape[1])), (num, self.W[1].shape[1]))\n",
    "            self.b.append(np.random.normal(0, np.sqrt(1 / self.K), self.K))\n",
    "    \n",
    "    def grad_desc(self, X, Y):\n",
    "        \n",
    "        self.wb_init(X.shape[0])\n",
    "        \n",
    "        update_W, update_b = self.W, self.b\n",
    "        v_W, v_b = self.W, self.b\n",
    "        m_W, m_b = self.W, self.b\n",
    "        \n",
    "        ey = np.zeros((self.K, self.n))\n",
    "        rows = np.arange(self.n)\n",
    "        ey[Y.T - 1, rows] = 1\n",
    "        \n",
    "        for t in range(1, self.num_epochs + 1):\n",
    "            \n",
    "            if(self.optimizer == 'sgd'):\n",
    "                for tt in range(0, ((self.n + self.batch_size - 1) // self.batch_size)):\n",
    "                    idx = np.random.randint(self.n, size = self.batch_size)\n",
    "                    ey = np.zeros((self.K, self.batch_size))\n",
    "                    rows = np.arange(self.batch_size)\n",
    "                    ey[Y.T[idx] - 1, rows] = 1\n",
    "                    Y_pred = self.forward_prop(X.T[idx, :].T)\n",
    "\n",
    "                    self.back_prop(Y_pred, ey)\n",
    "                    dW = self.gradW[::-1]\n",
    "                    db = self.gradB[::-1]\n",
    "\n",
    "                    update_W = [self.learning_rate * u for u in dW]\n",
    "                    update_b = [self.learning_rate * u for u in db]\n",
    "                    update_W = [u + v * self.weight_decay for u, v in zip(update_W, dW)]\n",
    "                    self.W = [u - v for u, v in zip(self.W, update_W)]\n",
    "                    self.b = [u - v for u, v in zip(self.b, update_b)]\n",
    "                continue\n",
    "                \n",
    "            Y_pred = self.forward_prop(X)\n",
    "            \n",
    "            if(self.optimizer == 'nesterov'):\n",
    "                W , b = self.W, self.b\n",
    "                ngamma = 0.9 # Hyperparameter\n",
    "                self.W = [u - ngamma * v for u, v in zip(self.W, update_W)]\n",
    "                self.b = [u - ngamma * v for u, v in zip(self.b, update_b)]\n",
    "            \n",
    "            self.back_prop(Y_pred, ey)\n",
    "            \n",
    "            \n",
    "            \n",
    "            dW = self.gradW[::-1]\n",
    "            db = self.gradB[::-1]\n",
    "            \n",
    "            if(self.optimizer == 'normal'):\n",
    "                update_W = [self.learning_rate * u for u in dW]\n",
    "                update_b = [self.learning_rate * u for u in db]\n",
    "\n",
    "                \n",
    "            elif(self.optimizer == 'momentum'):\n",
    "                mgamma = 0.9 # Hyperparameter\n",
    "                update_W = [mgamma * u + self.learning_rate * v for u, v in zip(update_W, dW)]\n",
    "                update_b = [mgamma * u + self.learning_rate * v for u, v in zip(update_b, db)]\n",
    "            \n",
    "            elif(self.optimizer == 'nesterov'):\n",
    "                self.W, self.b = W, b\n",
    "                update_W = [ngamma * u + self.learning_rate * v for u, v in zip(update_W, dW)]\n",
    "                update_b = [ngamma * u + self.learning_rate * v for u, v in zip(update_b, db)]\n",
    "            \n",
    "            elif(self.optimizer == 'rmsprop'):\n",
    "                rbeta = 0.9\n",
    "                epsilon = 0.1\n",
    "                v_W = [rbeta * u + (1 - rbeta) * (v ** 2) for u, v in zip(v_W, dW)]\n",
    "                v_b = [rbeta * u + (1 - rbeta) * (v ** 2) for u, v in zip(v_b, db)]\n",
    "                update_W = [(self.learning_rate * u) / np.sqrt(v + epsilon) for u, v in zip(dW, v_W)]\n",
    "                update_b = [(self.learning_rate * u) / np.sqrt(v + epsilon) for u, v in zip(db, v_b)]\n",
    "                \n",
    "            elif(self.optimizer == 'adam'):\n",
    "                abeta1 = 0.99\n",
    "                abeta2 = 0.999\n",
    "                epsilon = 0.1\n",
    "                m_W = [abeta1 * u + (1 - abeta1) * v for u, v in zip(m_W, dW)]\n",
    "                m_b = [abeta1 * u + (1 - abeta1) * v for u, v in zip(m_b, db)]\n",
    "                v_W = [abeta2 * u + (1 - abeta2) * (v ** 2) for u, v in zip(v_W, dW)]\n",
    "                v_b = [abeta2 * u + (1 - abeta2) * (v ** 2) for u, v in zip(v_b, db)]\n",
    "                update_W = [( (self.learning_rate * u) / (1 - abeta1 ** t) ) / np.sqrt( (v / (1 - abeta2 ** t)) + epsilon) for u, v in zip(m_W, v_W)]\n",
    "                update_b = [( (self.learning_rate * u) / (1 - abeta1 ** t) ) / np.sqrt( (v / (1 - abeta2 ** t)) + epsilon) for u, v in zip(m_b, v_b)]\n",
    "                 \n",
    "            elif(self.optimizer == 'nadam'):\n",
    "                nbeta1 = 0.99\n",
    "                nbeta2 = 0.999\n",
    "                epsilon = 0.1\n",
    "                m_W = [nbeta1 * u + (1 - nbeta1) * v for u, v in zip(m_W, dW)]\n",
    "                m_b = [nbeta1 * u + (1 - nbeta1) * v for u, v in zip(m_b, db)]\n",
    "                v_W = [nbeta2 * u + (1 - nbeta2) * (v ** 2) for u, v in zip(v_W, dW)]\n",
    "                v_b = [nbeta2 * u + (1 - nbeta2) * (v ** 2) for u, v in zip(v_b, db)]\n",
    "                update_W = [(self.learning_rate * (nbeta1 * u + (1 - nbeta1) * v)) / (1 - nbeta1 ** t) for u, v in zip(m_W, dW)]\n",
    "                update_W = [ (u) / np.sqrt( (v / (1 - nbeta2 ** t)) + epsilon) for u, v in zip(m_W, v_W)]\n",
    "                update_b = [(self.learning_rate * (nbeta1 * u + (1 - nbeta1) * v)) / (1 - nbeta1 ** t) for u, v in zip(m_b, db)]\n",
    "                update_b = [ (u) / np.sqrt( (v / (1 - nbeta2 ** t)) + epsilon) for u, v in zip(m_b, v_b)]\n",
    "            \n",
    "            \n",
    "            update_W = [u + v * self.weight_decay for u, v in zip(update_W, dW)]\n",
    "            \n",
    "            self.W = [u - v for u, v in zip(self.W, update_W)]\n",
    "            self.b = [u - v for u, v in zip(self.b, update_b)]\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.n = X_train.shape[0]\n",
    "        self.batch_size = min(self.batch_size, self.n)\n",
    "        K = np.max(Y_train) # Y_train must have values from 1 to K\n",
    "        self.layer_sizes[self.L - 1] = K\n",
    "        self.K = K\n",
    "        self.grad_desc(X_train.T, Y_train.T)\n",
    "        \n",
    "    def predict_proba(self, X_test):\n",
    "        return self.forward_prop(X_test.T).T\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return (self.predict_proba(X_test).argmax(axis=1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[ 1.,  2.,  0.,  8.],\n",
    " [ 6.,  3., 10.,  5.],\n",
    " [ 5.,  2.,  9.,  4.],\n",
    " [ 5.,  6.,  3.,  9.],\n",
    " [ 4.,  9.,  5.,  5.],\n",
    " [ 4.,  5.,  2.,  1.],\n",
    " [ 8.,  2.,  8.,  0.],\n",
    " [ 3.,  3.,  7.,  6.],\n",
    " [ 4.,  6.,  3.,  6.],\n",
    " [ 7.,  8.,  2.,  2.],\n",
    " [ 5.,  5., 10.,  2.]])\n",
    "X_test = np.array([[ 3.,  7.,  8.,  7.],\n",
    " [ 2.,  5.,  3.,  2.],\n",
    " [ 0.,  5.,  2.,  3.],\n",
    " [ 7.,  0.,  6.,  8.]])\n",
    "Y_train = np.array([3, 6, 5, 6, 6, 3, 4, 5, 5, 5, 6])\n",
    "Y_test = np.array([6, 3, 2, 5])\n",
    "\n",
    "#model = FNNClassifier(10, 2, activation = 'ReLU', optimizer = 'normal', num_epochs = 100)\n",
    "#model.fit(X, y)\n",
    "#print(model.predict_proba(X))\n",
    "#print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid', #'random',\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'layer_size': { \n",
    "            'values': [128]#[32, 64, 128] \n",
    "        },\n",
    "        'num_layers': { \n",
    "            'values': [5]#[3, 4, 5] \n",
    "        },\n",
    "        'activation': { \n",
    "            'values': ['sigmoid']#, 'tanh', 'ReLU'] \n",
    "        },\n",
    "        'optimizer': { \n",
    "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'] \n",
    "        },\n",
    "        'weight_decay': { \n",
    "            'values': [0]#[0, 0.0005, 0.5] \n",
    "        },\n",
    "        'batch_size': { \n",
    "            'values': [16]#[16, 32, 64] \n",
    "        },\n",
    "        'num_epochs': { \n",
    "            'values': [100]#[5, 10] \n",
    "        },\n",
    "        'learning_rate': { \n",
    "            'values': [0.001]#[0.001, 0.0001] \n",
    "        },\n",
    "        'weight_init': { \n",
    "            'values': ['Xavier']#['random', 'Xavier'] \n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ss3uuml7\n",
      "Sweep URL: https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, entity = 'raghuraman2000', project = 'cs6910_a1_0740')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    def_params = {\n",
    "        'layer_size' : 32,\n",
    "        'num_layers' : 4,\n",
    "        'activation' : 'ReLU',\n",
    "        'optimizer' : 'adam',\n",
    "        'weight_decay' : 0.0005,\n",
    "        'batch_size' : 32,\n",
    "        'num_epochs' : 5,\n",
    "        'learning_rate' : 0.001,\n",
    "        'weight_init' : 'random'\n",
    "    }\n",
    "    wandb.init(config = def_params)\n",
    "    config = wandb.config\n",
    "    model = FNNClassifier(**config)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predict = model.predict(X_test)\n",
    "    wandb.log({'accuracy': np.sum(Y_predict == Y_test) / Y_test.shape[0]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: mmtd9yur with config:\n",
      "wandb: \tactivation: sigmoid\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tlayer_size: 128\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tnum_epochs: 100\n",
      "wandb: \tnum_layers: 5\n",
      "wandb: \toptimizer: sgd\n",
      "wandb: \tweight_decay: 0\n",
      "wandb: \tweight_init: Xavier\n",
      "wandb: Currently logged in as: raghuraman2000 (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">iconic-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/mmtd9yur\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/mmtd9yur</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000738-mmtd9yur</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7988<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000738-mmtd9yur\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000738-mmtd9yur\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>128</td></tr><tr><td>_runtime</td><td>7</td></tr><tr><td>_timestamp</td><td>1615401465</td></tr><tr><td>_step</td><td>8</td></tr><tr><td>num_layers</td><td>5</td></tr><tr><td>activation</td><td>sigmoid</td></tr><tr><td>optimizer</td><td>sgd</td></tr><tr><td>weight_decay</td><td>0</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>weight_init</td><td>Xavier</td></tr><tr><td>accuracy</td><td>0.25</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>num_layers</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr><tr><td>batch_size</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">iconic-sweep-1</strong>: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/mmtd9yur\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/mmtd9yur</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: t899sytg with config:\n",
      "wandb: \tactivation: sigmoid\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tlayer_size: 128\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tnum_epochs: 100\n",
      "wandb: \tnum_layers: 5\n",
      "wandb: \toptimizer: momentum\n",
      "wandb: \tweight_decay: 0\n",
      "wandb: \tweight_init: Xavier\n",
      "wandb: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">stellar-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/t899sytg\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/t899sytg</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000750-t899sytg</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4988<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000750-t899sytg\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000750-t899sytg\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>128</td></tr><tr><td>_runtime</td><td>7</td></tr><tr><td>_timestamp</td><td>1615401477</td></tr><tr><td>_step</td><td>8</td></tr><tr><td>num_layers</td><td>5</td></tr><tr><td>activation</td><td>sigmoid</td></tr><tr><td>optimizer</td><td>momentum</td></tr><tr><td>weight_decay</td><td>0</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>weight_init</td><td>Xavier</td></tr><tr><td>accuracy</td><td>0.5</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>num_layers</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr><tr><td>batch_size</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">stellar-sweep-2</strong>: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/t899sytg\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/t899sytg</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Sweep Agent: Waiting for job.\n",
      "wandb: Job received.\n",
      "wandb: Agent Starting Run: m24cj0je with config:\n",
      "wandb: \tactivation: sigmoid\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tlayer_size: 128\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tnum_epochs: 100\n",
      "wandb: \tnum_layers: 5\n",
      "wandb: \toptimizer: nesterov\n",
      "wandb: \tweight_decay: 0\n",
      "wandb: \tweight_init: Xavier\n",
      "wandb: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">clear-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/m24cj0je\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/m24cj0je</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000816-m24cj0je</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16868<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000816-m24cj0je\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000816-m24cj0je\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>128</td></tr><tr><td>_runtime</td><td>6</td></tr><tr><td>_timestamp</td><td>1615401502</td></tr><tr><td>_step</td><td>8</td></tr><tr><td>num_layers</td><td>5</td></tr><tr><td>activation</td><td>sigmoid</td></tr><tr><td>optimizer</td><td>nesterov</td></tr><tr><td>weight_decay</td><td>0</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>weight_init</td><td>Xavier</td></tr><tr><td>accuracy</td><td>0.5</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>num_layers</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr><tr><td>batch_size</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">clear-sweep-3</strong>: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/m24cj0je\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/m24cj0je</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: tzyi3aab with config:\n",
      "wandb: \tactivation: sigmoid\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tlayer_size: 128\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tnum_epochs: 100\n",
      "wandb: \tnum_layers: 5\n",
      "wandb: \toptimizer: rmsprop\n",
      "wandb: \tweight_decay: 0\n",
      "wandb: \tweight_init: Xavier\n",
      "wandb: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lucky-sweep-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/tzyi3aab\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/tzyi3aab</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000828-tzyi3aab</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ac4167fd08b9>:159: RuntimeWarning: invalid value encountered in sqrt\n",
      "  update_W = [(self.learning_rate * u) / np.sqrt(v + epsilon) for u, v in zip(dW, v_W)]\n",
      "<ipython-input-2-ac4167fd08b9>:160: RuntimeWarning: invalid value encountered in sqrt\n",
      "  update_b = [(self.learning_rate * u) / np.sqrt(v + epsilon) for u, v in zip(db, v_b)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16804<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000828-tzyi3aab\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000828-tzyi3aab\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>128</td></tr><tr><td>_runtime</td><td>7</td></tr><tr><td>_timestamp</td><td>1615401515</td></tr><tr><td>_step</td><td>8</td></tr><tr><td>num_layers</td><td>5</td></tr><tr><td>activation</td><td>sigmoid</td></tr><tr><td>optimizer</td><td>rmsprop</td></tr><tr><td>weight_decay</td><td>0</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>weight_init</td><td>Xavier</td></tr><tr><td>accuracy</td><td>0.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>num_layers</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr><tr><td>batch_size</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">lucky-sweep-4</strong>: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/tzyi3aab\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/tzyi3aab</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: orrvibz4 with config:\n",
      "wandb: \tactivation: sigmoid\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tlayer_size: 128\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tnum_epochs: 100\n",
      "wandb: \tnum_layers: 5\n",
      "wandb: \toptimizer: adam\n",
      "wandb: \tweight_decay: 0\n",
      "wandb: \tweight_init: Xavier\n",
      "wandb: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">golden-sweep-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/orrvibz4\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/orrvibz4</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000840-orrvibz4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ac4167fd08b9>:170: RuntimeWarning: invalid value encountered in sqrt\n",
      "  update_W = [( (self.learning_rate * u) / (1 - abeta1 ** t) ) / np.sqrt( (v / (1 - abeta2 ** t)) + epsilon) for u, v in zip(m_W, v_W)]\n",
      "<ipython-input-2-ac4167fd08b9>:171: RuntimeWarning: invalid value encountered in sqrt\n",
      "  update_b = [( (self.learning_rate * u) / (1 - abeta1 ** t) ) / np.sqrt( (v / (1 - abeta2 ** t)) + epsilon) for u, v in zip(m_b, v_b)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1480<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000840-orrvibz4\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000840-orrvibz4\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>128</td></tr><tr><td>_runtime</td><td>7</td></tr><tr><td>_timestamp</td><td>1615401527</td></tr><tr><td>_step</td><td>8</td></tr><tr><td>num_layers</td><td>5</td></tr><tr><td>activation</td><td>sigmoid</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>weight_decay</td><td>0</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>weight_init</td><td>Xavier</td></tr><tr><td>accuracy</td><td>0.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>num_layers</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr><tr><td>batch_size</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">golden-sweep-5</strong>: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/orrvibz4\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/orrvibz4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: ff63h109 with config:\n",
      "wandb: \tactivation: sigmoid\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tlayer_size: 128\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tnum_epochs: 100\n",
      "wandb: \tnum_layers: 5\n",
      "wandb: \toptimizer: nadam\n",
      "wandb: \tweight_decay: 0\n",
      "wandb: \tweight_init: Xavier\n",
      "wandb: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">upbeat-sweep-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/sweeps/ss3uuml7</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/ff63h109\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/ff63h109</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000852-ff63h109</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ac4167fd08b9>:182: RuntimeWarning: invalid value encountered in sqrt\n",
      "  update_W = [ (u) / np.sqrt( (v / (1 - nbeta2 ** t)) + epsilon) for u, v in zip(m_W, v_W)]\n",
      "<ipython-input-2-ac4167fd08b9>:184: RuntimeWarning: invalid value encountered in sqrt\n",
      "  update_b = [ (u) / np.sqrt( (v / (1 - nbeta2 ** t)) + epsilon) for u, v in zip(m_b, v_b)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10208<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000852-ff63h109\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\R Raghu Raman\\Desktop\\cs6910\\DeepLearning\\wandb\\run-20210311_000852-ff63h109\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>128</td></tr><tr><td>_runtime</td><td>6</td></tr><tr><td>_timestamp</td><td>1615401538</td></tr><tr><td>_step</td><td>8</td></tr><tr><td>num_layers</td><td>5</td></tr><tr><td>activation</td><td>sigmoid</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>weight_decay</td><td>0</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>weight_init</td><td>Xavier</td></tr><tr><td>accuracy</td><td>0.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>layer_sizes</td><td>▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>num_layers</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr><tr><td>batch_size</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">upbeat-sweep-6</strong>: <a href=\"https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/ff63h109\" target=\"_blank\">https://wandb.ai/raghuraman2000/cs6910_a1_0740/runs/ff63h109</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Sweep Agent: Waiting for job.\n",
      "wandb: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
